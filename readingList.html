
    <html>
    <head>
        <title>Guided Reading</title>
        <style>
            body {
                font-family: 'Arial', sans-serif;
                margin: 40px;
                padding: 0;
                background-color: #f4f4f4;
            }
            h1 {
                color: #2c3e50;
                text-align: center;
                margin-bottom: 30px;
            }
            ul {
                list-style-type: none;
                padding: 0;
                max-width: 800px;
                margin: 0 auto;
                background-color: #fff;
                box-shadow: 0 0 10px rgba(0,0,0,0.1);
                border-radius: 8px;
                padding: 20px;
            }
            li {
                padding: 10px;
                border-bottom: 1px solid #ccc;
            }
            li:last-child {
                border-bottom: none;
            }
            a {
                text-decoration: none;
                color: #3498db;
                font-weight: bold;
            }
            a:hover {
                color: #2980b9;
            }
        </style>
    </head>
    <body>
        <h1>Reading List for Freshman</h1>
        <ul>
    <li><a href=".DS_Store" target="_blank">.DS_Store</a></li><li><a href="2017.08.28%20-%20PPO%20-%20Proximal%20Policy%20Optimization%20Algorithms.pdf" target="_blank">2017.08.28 - PPO - Proximal Policy Optimization Algorithms.pdf</a></li><li><a href="2017.12.06%20-%20Transformer%20-%20Attention%20Is%20All%20You%20Need.pdf" target="_blank">2017.12.06 - Transformer - Attention Is All You Need.pdf</a></li><li><a href="2018.06.11%20-%20GPT%20-%20Improving%20Language%20Understanding%20by%20Generative%20Pre-Training.pdf" target="_blank">2018.06.11 - GPT - Improving Language Understanding by Generative Pre-Training.pdf</a></li><li><a href="2018.10.05%20-%20TRANX%20-%20A%20Transition-based%20Neural%20Abstract%20Syntax%20Parser%20for%20Semantic%20Parsing%20and%20Code%20Generation.pdf" target="_blank">2018.10.05 - TRANX - A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation.pdf</a></li><li><a href="2019%20-%20%E8%BD%AF%E4%BB%B6%E5%AD%A6%E6%8A%A5%20-%20%E8%BD%AF%E4%BB%B6%E7%BC%BA%E9%99%B7%E9%A2%84%E6%B5%8B%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95.pdf" target="_blank">2019 - 软件学报 - 软件缺陷预测技术研究进展.pdf</a></li><li><a href="2019.06.02%20Transformer-XL%20-%20Attentive%20Language%20Models%20Beyond%20a%20Fixed-Length%20Context.pdf" target="_blank">2019.06.02 Transformer-XL - Attentive Language Models Beyond a Fixed-Length Context.pdf</a></li><li><a href="2019.11.06%20-%20Multi-query%20attention%20%28MQA%29%20-%20Fast%20Transformer%20Decoding%20-%20One%20Write-Head%20is%20All%20You%20Need.pdf" target="_blank">2019.11.06 - Multi-query attention (MQA) - Fast Transformer Decoding - One Write-Head is All You Need.pdf</a></li><li><a href="2020%20-%20%20Linear%20Transformer%20-%20Transformers%20are%20RNNs%20-%20Fast%20Autoregressive%20Transformers%20with%20Linear%20Attention.pdf" target="_blank">2020 -  Linear Transformer - Transformers are RNNs - Fast Autoregressive Transformers with Linear Attention.pdf</a></li><li><a href="2020.01.08%20-%20RLHF%20-%20Fine-Tuning%20Language%20Models%20from%20Human%20Preferences.pdf.pdf" target="_blank">2020.01.08 - RLHF - Fine-Tuning Language Models from Human Preferences.pdf.pdf</a></li><li><a href="2020.07.22%20-%20GPT-3%20-%20NeurIPS%20-%20Language%20Models%20are%20Few-Shot%20Learners.pdf" target="_blank">2020.07.22 - GPT-3 - NeurIPS - Language Models are Few-Shot Learners.pdf</a></li><li><a href="2020.12.06%20-%20GPT-3%20-%20NeurIPS%20-%20Language%20Models%20are%20Few-Shot%20Learners.pdf" target="_blank">2020.12.06 - GPT-3 - NeurIPS - Language Models are Few-Shot Learners.pdf</a></li><li><a href="2020.12.22%20-%20arXiv%20-%20Intrinsic%20dimensionality%20explains%20the%20effectiveness%20of%20language%20model%20fine-tuning.pdf" target="_blank">2020.12.22 - arXiv - Intrinsic dimensionality explains the effectiveness of language model fine-tuning.pdf</a></li><li><a href="2021.07.07%20-%20Codex%20-%20Evaluating%20Large%20Language%20Models%20Trained%20on%20Code.pdf" target="_blank">2021.07.07 - Codex - Evaluating Large Language Models Trained on Code.pdf</a></li><li><a href="2021.08.11%20-%20Pre-Trained%20Models-%20Past%2C%20Present%20and%20Future.pdf" target="_blank">2021.08.11 - Pre-Trained Models- Past, Present and Future.pdf</a></li><li><a href="2021.08.16%20-%20MBPP%20-%20Program%20Synthesis%20with%20Large%20Language%20Models.pdf" target="_blank">2021.08.16 - MBPP - Program Synthesis with Large Language Models.pdf</a></li><li><a href="2021.10.16%20-%20LoRA-%20Low-Rank%20Adaptation%20of%20Large%20Language%20Models.pdf" target="_blank">2021.10.16 - LoRA- Low-Rank Adaptation of Large Language Models.pdf</a></li><li><a href="2022.01.06%20-%20Grokking%20-%01%20Generalization%01%20Beyond%01%20Overfitting%01%20on%01%20Small%01%20Algorithmic%01%20Datasets%01.pdf" target="_blank">2022.01.06 - Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets.pdf</a></li><li><a href="2022.03.04%20-%20InstructGPT%20-%20Training%20Language%20Models%20to%20Follow%20Instructions%20with%20Human%20Feedback.pdf" target="_blank">2022.03.04 - InstructGPT - Training Language Models to Follow Instructions with Human Feedback.pdf</a></li><li><a href="2022.03.26%20-%20arXiv%20-%20A%20Road%20Map%20of%20Big%20Models.pdf" target="_blank">2022.03.26 - arXiv - A Road Map of Big Models.pdf</a></li><li><a href="2022.03.29%20-%20Chinchilla%20-%20Training%20Compute-Optimal%20Large%20Language%20Models.pdf" target="_blank">2022.03.29 - Chinchilla - Training Compute-Optimal Large Language Models.pdf</a></li><li><a href="2022.04.12%20-%20InCoder-%20A%20Generative%20Model%20for%20Code%20Infilling%20and%20Synthesis.pdf" target="_blank">2022.04.12 - InCoder- A Generative Model for Code Infilling and Synthesis.pdf</a></li><li><a href="2022.06.15%20-%20Emergence%20-%20arXiv%20-%20Emergent%20Abilities%20of%20Large%20Language%20Models.pdf" target="_blank">2022.06.15 - Emergence - arXiv - Emergent Abilities of Large Language Models.pdf</a></li><li><a href="2022.07.21%20-%20CodeT%20-%20Code%20Generation%20with%20Generated%20Tests.pdf" target="_blank">2022.07.21 - CodeT - Code Generation with Generated Tests.pdf</a></li><li><a href="2022.07.28%20-%20Infilling%20-%20Efficient%20Training%20of%20Language%20Models%20to%20Fill%20in%20the%20Middle.pdf" target="_blank">2022.07.28 - Infilling - Efficient Training of Language Models to Fill in the Middle.pdf</a></li><li><a href="2022.08.05%20%20-%20Efficiently%20Modeling%20Long%20Sequences%20with%20Structured%20State%20Spaces.pdf" target="_blank">2022.08.05  - Efficiently Modeling Long Sequences with Structured State Spaces.pdf</a></li><li><a href="2022.08.09%20-%20RoPE-%20RoFormer%20-%20Enhanced%20Transformer%20with%20Rotary%20Position%20Embedding.pdf" target="_blank">2022.08.09 - RoPE- RoFormer - Enhanced Transformer with Rotary Position Embedding.pdf</a></li><li><a href="2022.10.05%20-%20GLM-130B%20-%20An%20Open%20Bilingual%20Pre-trained%20Model.pdf" target="_blank">2022.10.05 - GLM-130B - An Open Bilingual Pre-trained Model.pdf</a></li><li><a href="2022.10.22%20-%20A%20Survey%20of%20Transformers.pdf" target="_blank">2022.10.22 - A Survey of Transformers.pdf</a></li><li><a href="2022.10.26%20-%20Instruction%20Tuning%20-%20arXiv%20-%20InstructDial-%20Improving%20Zero%20and%20Few-shot%20Generalization%20in%20Dialogue%20through%20Instruction%20Tuning.pdf" target="_blank">2022.10.26 - Instruction Tuning - arXiv - InstructDial- Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning.pdf</a></li><li><a href="2022.11.20%20-%20The%20Stack%20-%20The%20Stack-%203%20TB%20of%20permissively%20licensed%20source%20code.pdf" target="_blank">2022.11.20 - The Stack - The Stack- 3 TB of permissively licensed source code.pdf</a></li><li><a href="2022.12.09%20-%20NL2Code%20Survey%20-%20When%20Neural%20Model%20Meets%20NL2Code-%20A%20Survey%20.pdf" target="_blank">2022.12.09 - NL2Code Survey - When Neural Model Meets NL2Code- A Survey .pdf</a></li><li><a href="2022.12.20%20-%20Self-Instruct%20-%20Aligning%20Language%20Model%20with%20Self%20Generated%20Instructions.pdf" target="_blank">2022.12.20 - Self-Instruct - Aligning Language Model with Self Generated Instructions.pdf</a></li><li><a href="2022.12.21%20-%20Zhifang%20Sui%20-%20Why%20Can%20GPT%20Learn%20In-Context%3F%20Language%20Models%20Secretly%20Perform%20Gradient%20Descent%20as%20Meta-Optimizers.pdf" target="_blank">2022.12.21 - Zhifang Sui - Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers.pdf</a></li><li><a href="2023%20-%20DPO%20-%20NeurIPS%20-%20Direct%20Preference%20Optimization-%20Your%20Language%20Model%20is%20Secretly%20a%20Reward%20Model.pdf" target="_blank">2023 - DPO - NeurIPS - Direct Preference Optimization- Your Language Model is Secretly a Reward Model.pdf</a></li><li><a href="2023%20-%20%E8%BD%AF%E4%BB%B6%E5%AD%A6%E6%8A%A5%20-%20%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%BA%90%E4%BB%A3%E7%A0%81%E7%BC%BA%E9%99%B7%E6%A3%80%E6%B5%8B%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0.pdf" target="_blank">2023 - 软件学报 - 基于深度学习的源代码缺陷检测研究综述.pdf</a></li><li><a href="2023%20-%20%E8%BD%AF%E4%BB%B6%E5%AD%A6%E6%8A%A5%20-%20%E5%BC%80%E6%BA%90%E8%BD%AF%E4%BB%B6%E4%BE%9B%E5%BA%94%E9%93%BE%E5%AE%89%E5%85%A8%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0.pdf" target="_blank">2023 - 软件学报 - 开源软件供应链安全研究综述.pdf</a></li><li><a href="2023.02.01%20-%20CoderEval-%20A%20Benchmark%20of%20Pragmatic%20Code%20Generation%20with%20Generative%20Pre-trained%20Models.pdf" target="_blank">2023.02.01 - CoderEval- A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models.pdf</a></li><li><a href="2023.02.18%20-%20arXiv%20-%20A%20Comprehensive%20Survey%20on%20Pretrained%20Foundation%20Models-%20A%20History%20from%20BERT%20to%20ChatGPT.pdf" target="_blank">2023.02.18 - arXiv - A Comprehensive Survey on Pretrained Foundation Models- A History from BERT to ChatGPT.pdf</a></li><li><a href="2023.02.20%20-%20Toolformer%20-%20%E5%BC%A0%E5%85%8B%E9%A9%B0%20-%20Language%20Models%20Can%20Teach%20Themselves%20to%20Use%20Tools.pdf" target="_blank">2023.02.20 - Toolformer - 张克驰 - Language Models Can Teach Themselves to Use Tools.pdf</a></li><li><a href="2023.02.27%20-%20Llama%20-%20Open%20and%20Efficient%20Foundation%20Language%20Models.pdf" target="_blank">2023.02.27 - Llama - Open and Efficient Foundation Language Models.pdf</a></li><li><a href="2023.02.27%20-%20arXiv%20-%20LLaMA-%20Open%20and%20Efficient%20Foundation%20Language%20Models.pdf" target="_blank">2023.02.27 - arXiv - LLaMA- Open and Efficient Foundation Language Models.pdf</a></li><li><a href="2023.03.02%20-%20Parameter-efficient%20Fine-tuning%20of%20Large-scale%20Pre-trained%20Language%20Models.pdf" target="_blank">2023.03.02 - Parameter-efficient Fine-tuning of Large-scale Pre-trained Language Models.pdf</a></li><li><a href="2023.03.15%20-%20GPT-4%20-%20Technical%20Report.pdf" target="_blank">2023.03.15 - GPT-4 - Technical Report.pdf</a></li><li><a href="2023.03.22%20-%20Sparks%20of%20Artificial%20General%20Intelligence%20-%20Early%20experiments%20with%20GPT-4.pdf" target="_blank">2023.03.22 - Sparks of Artificial General Intelligence - Early experiments with GPT-4.pdf</a></li><li><a href="2023.05.07%20-%20arXiv%20-%20A%20Survey%20of%20Large%20Language%20Models%20%20-%20Jirong%20Wen.pdf" target="_blank">2023.05.07 - arXiv - A Survey of Large Language Models  - Jirong Wen.pdf</a></li><li><a href="2023.05.09%20-%20StarCoder%20-%20May%20the%20Source%20Code%20be%20with%20You.pdf" target="_blank">2023.05.09 - StarCoder - May the Source Code be with You.pdf</a></li><li><a href="2023.05.18%20-%20LIMA%20-%20Less%20Is%20More%20for%20Alignment.pdf" target="_blank">2023.05.18 - LIMA - Less Is More for Alignment.pdf</a></li><li><a href="2023.05.22%20-%20Grouped-Query%20Attention%20-%20GQA%20-%20Training%20Generalized%20Multi-Query%20Transformer%20Models%20from%20Multi-Head%20Checkpoints.pdf" target="_blank">2023.05.22 - Grouped-Query Attention - GQA - Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.pdf</a></li><li><a href="2023.06.14%20-%20WizardCoder-%20Empowering%20Code%20Large%20Language%20%20Models%20with%20Evol-Instruct.pdf" target="_blank">2023.06.14 - WizardCoder- Empowering Code Large Language  Models with Evol-Instruct.pdf</a></li><li><a href="2023.07.17%20-%20LouJianguang%20%20-%20Large%20Language%20Models%20Meet%20NL2Code-%20A%20Survey.pdf" target="_blank">2023.07.17 - LouJianguang  - Large Language Models Meet NL2Code- A Survey.pdf</a></li><li><a href="2023.07.19%20-%20Llama%202%20-%20Open%20Foundation%20and%20Fine-Tuned%20Chat%20Models.pdf" target="_blank">2023.07.19 - Llama 2 - Open Foundation and Fine-Tuned Chat Models.pdf</a></li><li><a href="2023.08.21%20-%20%E4%B8%8D%E6%80%8E%E4%B9%88%E6%A0%B7%20-%20Exploring%20Parameter-Efficient%20Fine-Tuning%20Techniques%20for%20Code%20Generation%20with%20Large%20Language%20Models.pdf" target="_blank">2023.08.21 - 不怎么样 - Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models.pdf</a></li><li><a href="2023.08.25%20-%20Code%20Llama-%20Open%20Foundation%20Models%20for%20Code.pdf" target="_blank">2023.08.25 - Code Llama- Open Foundation Models for Code.pdf</a></li><li><a href="2023.09.21%20-%20Instruction%20Tuning%20for%20Large%20Language%20Models-%20A%20Survey.pdf" target="_blank">2023.09.21 - Instruction Tuning for Large Language Models- A Survey.pdf</a></li><li><a href="2023.10.10%20-%20CodeFuse%20-13B-%20A%20Pretrained%20Multi-lingual%20Code%20Large%20Language%20Model.pdf" target="_blank">2023.10.10 - CodeFuse -13B- A Pretrained Multi-lingual Code Large Language Model.pdf</a></li><li><a href="2023.11.03%20-%20arXiv%20-%20SIMPLIFYING%20TRANSFORMER%20BLOCKS.pdf" target="_blank">2023.11.03 - arXiv - SIMPLIFYING TRANSFORMER BLOCKS.pdf</a></li><li><a href="2023.11.24%20-%20arXiv%20-%20A%20Survey%20of%20Large%20Language%20Models%20%20-%20Jirong%20Wen.pdf" target="_blank">2023.11.24 - arXiv - A Survey of Large Language Models  - Jirong Wen.pdf</a></li><li><a href="2023.12.01%20-%20arXiv%20-%20Mamba-%20Linear-Time%20Sequence%20Modeling%20with%20Selective%20State%20Spaces.pdf" target="_blank">2023.12.01 - arXiv - Mamba- Linear-Time Sequence Modeling with Selective State Spaces.pdf</a></li><li><a href="2023.12.23%20-%20arXiv%20-%20GQA-%20Training%20Generalized%20Multi-Query%20Transformer%20Models%20from%20Multi-Head%20Checkpoints.pdf" target="_blank">2023.12.23 - arXiv - GQA- Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.pdf</a></li><li><a href="2024.02.01%20-%20Instruction%20Finetuning%20%20-%20Scaling%20Instruction-Finetuned%20Language%20Models.pdf" target="_blank">2024.02.01 - Instruction Finetuning  - Scaling Instruction-Finetuned Language Models.pdf</a></li><li><a href="2024.02.20%20-%20Large%20Language%20Models%20-%20A%20Survey.pdf" target="_blank">2024.02.20 - Large Language Models - A Survey.pdf</a></li><li><a href="2024.0302%20-%20arXiv%20-%20The%20Era%20of%201-bit%20LLMs_%20All%20Large%20Language%20Models%20are%20in%201.58%20Bits.pdf" target="_blank">2024.0302 - arXiv - The Era of 1-bit LLMs_ All Large Language Models are in 1.58 Bits.pdf</a></li><li><a href="2024.04.10%20-%20DeepNet-%20Scaling%20Transformers%20to%201%2C000%20Layers.pdf" target="_blank">2024.04.10 - DeepNet- Scaling Transformers to 1,000 Layers.pdf</a></li><li><a href="2024.05.20%20-%20arXiv%20-%20KAN-%20Kolmogorov%E2%80%93Arnold%20Networks.pdf" target="_blank">2024.05.20 - arXiv - KAN- Kolmogorov–Arnold Networks.pdf</a></li><li><a href="2024.05.23%20-%20Grokked%20Transformers%20are%20Implicit%20Reasoners-%20A%20Mechanistic%20Journey%20to%20the%20Edge%20of%20Generalization.pdf" target="_blank">2024.05.23 - Grokked Transformers are Implicit Reasoners- A Mechanistic Journey to the Edge of Generalization.pdf</a></li><li><a href="2024.05.23%20-%20SimPO-%20Simple%20Preference%20Optimization%20with%20a%20Reference-Free%20Reward.pdf" target="_blank">2024.05.23 - SimPO- Simple Preference Optimization with a Reference-Free Reward.pdf</a></li><li><a href="2024.05.31%20-%20Mamba-%20Linear-Time%20Sequence%20Modeling%20with%20Selective%20State%20Spaces.pdf" target="_blank">2024.05.31 - Mamba- Linear-Time Sequence Modeling with Selective State Spaces.pdf</a></li><li><a href="2024.06.12%20-%20KAN-%20Kolmogorov-Arnold%20Networks.pdf" target="_blank">2024.06.12 - KAN- Kolmogorov-Arnold Networks.pdf</a></li><li><a href="2024.06.26%20-%20Case-based%20or%20Rule-based-%20How%20Do%20Transformers%20Do%20the%20Math%3F.pdf" target="_blank">2024.06.26 - Case-based or Rule-based- How Do Transformers Do the Math?.pdf</a></li>
        </ul>
    </body>
    </html>
    